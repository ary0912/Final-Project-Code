# ğŸ“˜ Cognitive Load Detection via Eye Movements

A deep learning pipeline to detect cognitive load from binocular eye-tracking data using temporal models and explainable AI. Designed for real-world applications such as education, healthcare, and cognitive screening.

---

## ğŸ‘¤ About Me

**Aryan Lodha**  
Incoming Graduate, MSc Data Science â€” University of Bristol  
ğŸ“§ aryanlodha881@gmail.com  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/aryan-lodha-31b6361b8/) â€¢ [GitHub](https://github.com/aryanlodha881](https://github.com/ary0912)

**Role in this project:**
- Designed end-to-end pipeline
- Built feature engineering modules
- Implemented and tuned classical + deep models
- Led explainability and visual storytelling

---

## ğŸ§ª Model Performance

| Model              | Accuracy | F1 Score | ROC-AUC |
|-------------------|----------|----------|---------|
| Logistic Regression | 84.6%   | 0.83     | 0.85    |
| Random Forest       | 93.2%   | 0.93     | 0.94    |
| CNN + LSTM          | 95.5%   | 0.96     | 0.96    |
| TCN                 | 96.9%   | 0.97     | 0.97    |
| **Hybrid TCN+ViT**  | **98.8%** | **0.99** | **0.99** |

âœ… Subject-wise train/test split used (no identity leakage)  
âœ… 5-fold cross-validated

---

## ğŸ” Explainability

### SHAP (for classical models):
- Blink rate and gaze velocity dominate
- Fixation entropy useful in high-load states

### GradCAM++ (for deep models):
- Temporal saliency maps show peaks aligned with cognitive effort
- Attention on blink bursts, pupil constriction, and scan paths

---

## ğŸ“Š Visual Insights

- ğŸ“‰ **Entropy**: right-skewed under high load
- ğŸ” **Blink suppression** indicates sustained attention
- ğŸ”­ **Jerkiness** flags decision conflicts
- ğŸ”— **Scatter plots** reveal clustering in feature space

---

## ğŸ¤¯ Challenges & Solutions

| Challenge                        | Mitigation                                      |
|----------------------------------|--------------------------------------------------|
| Noisy or missing gaze frames     | Linear interpolation + blink-based masking       |
| Subject overfitting              | Subject-wise data splits                         |
| Redundant features               | PCA and SHAP for dimensionality reduction        |
| Deep learning black-box issues   | SHAP + GradCAM++ for transparency                |

---

## ğŸŒ± Future Work

- ğŸ“± Real-time deployment via mobile/tablet
- ğŸ§¬ Cognitive disorder screening (Alzheimerâ€™s, MCI)
- ğŸ¥ Eye + EEG + facial EMG fusion
- ğŸŒ Cloud-hosted demos (Flask/Streamlit)

---

## ğŸ§  Dataset

**EM-COGLOAD (Miles et al., 2024)**  
Raw binocular eye-tracking across videos under low and high cognitive load  
- Trials: `data/trials/`
- Repeats: `data/repeats/`  
- Format: `<participant_id>_<task_id>.csv`  
  - `_0` = Low Load (Video A), `_2` = High Load (Video A)  
  - `_1` = Low Load (Video B), `_3` = High Load (Video B)

---

## âš™ï¸ Repository Structure

cognitive-load-eye-tracking/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ trials/              # Raw trial data (CSV files like 021_0.csv, 045_3.csv)
â”‚   â””â”€â”€ repeats/             # Repeated trials for intra-subject validation
â”‚
â”œâ”€â”€ explore3.ipynb           # Feature engineering (velocity, blink rate, entropy, etc.)
â”œâ”€â”€ explore4.ipynb           # Classical ML (Logistic Regression, Random Forest, SHAP)
â”œâ”€â”€ explore5.ipynb           # Deep learning (TCN, ViT), GradCAM++ explanations
â”‚
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # Project documentation (you're here!)
â””â”€â”€ LICENSE                  # MIT license for open-source reuse


---

## ğŸ“ Acknowledgements

- Dr. Gabriella Miles â€” Research Supervisor  
- Miles et al. (2024) â€” For the EM-COGLOAD dataset  
- TensorFlow, SHAP, GradCAM++ â€” Open-source frameworks  
- University of Bristol â€” Research support
---

_This project demonstrates deep domain knowledge in cognitive science, signal processing, and machine learning. If youâ€™re hiring for AI, healthcare tech, or applied ML roles â€” letâ€™s talk._
